{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aq_input Tips and Samples\n",
    "\n",
    "\n",
    "Aq-input is a set of options and its attributes that you can specify in order to control data input behaviors. Aq-input applies to all of aq_tools (aq_commands), so it's important to understand and become comfortable with its use.\n",
    "\n",
    "This notebook goes over some of aq_input's options and it's sample usages briefly. For detailed and exact spec and syntactic explanations, please refer to [aq_input - essentia documentation portal](http://auriq.com/documentation/source/reference/manpages/aq-input.html).\n",
    "This notebook is based on AQ Tools version: 2.0.1-1, and you can check the version on your system by running `ess --version`.\n",
    "\n",
    "\n",
    "### Prerequisites\n",
    "Users are assumed to be equipped with decent knowledge of\n",
    "- bash commands\n",
    "- intuitive knowledge of `aq_pp` command. ([link to aq_pp official documentation](http://auriq.com/documentation/source/reference/manpages/aq_pp.html))\n",
    "\n",
    "\n",
    "## Major Structure\n",
    "Below is the general structure of the aq-input, which is composed of input spec and column spec\n",
    "\n",
    "`aq_command ... -f[,attributes] fileName [more filenames...] -d columnSpec`\n",
    "\n",
    "There are 2 major components in input specification of aq_tool. \n",
    "\n",
    "* `-f`: file input: specifies data source's type, and its format as well as input behaviors, and data input's name(file/pipe name).\n",
    "* `-d`: column specs: specifies numbers, names and data types of columns.\n",
    "\n",
    "Before getting into examples, let us go through variations of accepted attributes for both options.\n",
    "**If you've already read the documentation and are familiar with these options, go ahead and skip this section and jump right into [data used for this sample](#data)**.\n",
    "\n",
    "Otherwise let's take a look at some commonly used options here.\n",
    "\n",
    "\n",
    "### `-f` Basic file reading options\n",
    "\n",
    "By this option, we can specify 3 things + some extra, such as input sorce type, input format and error handling. \n",
    "\n",
    "#### input source type\n",
    "- file\n",
    "- stream from standard output (std)\n",
    "- named pipe\n",
    "- stream from connection to listener\n",
    "\n",
    "#### input format (column separator) selection\n",
    "- `csv`: the default option.\n",
    "- `sep`: this option lets users specify their own separater, as long as it is [single byte character](https://www.quora.com/What-is-the-difference-between-single-byte-or-multibyte-unicode)\n",
    "- `div`: used with `sep` option in column spec, this let's us specify different separators for each columns. \n",
    "- `tab`: html table format\n",
    "- `jsn`: allows us to input json formatted files. \n",
    "- `xml`: xml formatted file input.\n",
    "- `bin`: aq_pp's original input format\n",
    "- `aq`: from another aq_tool outputtting aq. No column spec needed. \n",
    "\n",
    "\n",
    "#### Error Handling\n",
    "- `eok`: Make error non critical, resulting in skipping the rows with error.\n",
    "\n",
    "#### Others\n",
    "- `esc`: input '\\' character in the data as escape character.\n",
    "- `Num`: lines / byhtes to skip\n",
    "- `bz`: buffer size\n",
    "\n",
    "\n",
    "### `-d` Column specs\n",
    "\n",
    "Column spec specifies what data type will the each column in the data be interpreted by, as well as each column's name. \n",
    "Therefore, column spec has a format of \n",
    "``` bash\n",
    "-d dtype[,attributes]:colName dtype2[,attributes]:colName2 ... \n",
    "```\n",
    "\n",
    "\n",
    "#### Generic Col Spec\n",
    "\n",
    "Supported data types are \n",
    "- `S`: string\n",
    "- `F`: Double precision floating point\n",
    "- `L`: unsigned integer\n",
    "- `LS`: 64 bit signed integer\n",
    "- `I`: 32 bit unsigned integer\n",
    "- `IS`: 32 bit signed integer\n",
    "- `IP`: v4/v6 address\n",
    "- `X`: placeholder for unwanted input column.\n",
    "\n",
    "And some of the attributes that might come in handy are...\n",
    "- `trm`: trim leading/trailing spaces from field value\n",
    "- `lo`, `up`: convert a string field value to lower or upper case. \n",
    "\n",
    "There's more attributes available, take a look at the documentation for further details.\n",
    "\n",
    "Now we are equipped with the basic knowledge of input specifications, let's get started on examples.\n",
    "\n",
    "<a id='data'></a>\n",
    "## Data \n",
    "\n",
    "We will start with the simplest inputting of csv file, consists of numeric and string columns.\n",
    "This is a data of street price of cannabis across different states.(The data is modified for simplicity)\n",
    "\n",
    "Data looks like following, \n",
    "\n",
    "State|HighQ|HighQN|date\n",
    "---|---|---|---|\n",
    "Alabama|339.06|1042|2014-01-01\n",
    "Alaska|288.75|252|2014-01-01\n",
    "Arizona|303.31|1941|2014-01-01\n",
    "Arkansas|361.85000000000002|576|2014-01-01\n",
    "California|248.78|12096|2014-01-01\n",
    "Colorado|236.31|2161|2014-01-01\n",
    "Connecticut|347.89999999999998|1294|2014-01-01\n",
    "Delaware|373.18000000000001|347|2014-01-01\n",
    "District of Columbia|352.25999999999999|433|2014-01-01\n",
    "Florida|306.43000000000001|6506|2014-01-01\n",
    "\n",
    "**Actual file looks like below**<br>\n",
    "```\n",
    "State,HighQ,HighQN,MedQ,MedQN,LowQ,LowQN,date\n",
    "Alabama,339.06,1042,198.63999999999999,933,149.49000000000001,123,2014-01-01\n",
    "Alaska,288.75,252,260.60000000000002,297,388.57999999999998,26,2014-01-01\n",
    "Arizona,303.31,1941,209.34999999999999,1625,189.44999999999999,222,2014-01-01\n",
    "Arkansas,361.85000000000002,576,185.62,544,125.87,112,2014-01-01\n",
    "California,248.78,12096,193.56,12812,192.91999999999999,778,2014-01-01\n",
    "Colorado,236.31,2161,195.28999999999999,1728,213.5,128,2014-01-01\n",
    "Connecticut,347.89999999999998,1294,273.97000000000003,1316,257.36000000000001,91,2014-01-01\n",
    "Delaware,373.18000000000001,347,226.25,273,199.88,34,2014-01-01\n",
    "District of Columbia,352.25999999999999,433,295.67000000000002,349,213.72,39,2014-01-01\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### Columns \n",
    "- 2 **string** columns, `State` and `date`\n",
    "- 1 **float** column, `HighQ` and\n",
    "- 1 **int** columns, `HighQN`\n",
    "\n",
    "<a id='usage_examples'></a>\n",
    "## Basic Usage Examples\n",
    "What we need to read in this data with aq-input are followings:\n",
    "- [input spec](#input_spec)\n",
    "    * file name \n",
    "    * attributes\n",
    "- [column spec](#column_spec)\n",
    "\n",
    "<a id='input_spec'></a>\n",
    "### Input Spec\n",
    "With the above data, few things needs to be considered for input spec.\n",
    "1. attributes\n",
    "2. path to the fileName, which is `data/aq_input/partial_cannibas_price.csv`\n",
    "\n",
    "`... -f,+1 data/aq_input/partial_cannibas_price.csv ...`\n",
    "\n",
    "For attributes, we are specifying that we'll skip the first row of the file because it is a header, by providing `+1`. Because csv format is the default for input spec, we don't have to provide it explicitly.\n",
    "\n",
    "After the attribute, we can simply provide the path to the file name.\n",
    "\n",
    "<a id=\"column_spec\"></a>\n",
    "### Column Spec\n",
    "This needs to be in the same order as the actual columns in the data. Therefore, considering the data types we've looked at earlier, we can specify column specs with data types and column names.\n",
    "\n",
    "`-d S:state F:highQ I:highQN S:date`\n",
    "\n",
    "Note that data type's capitailzation does not matter, and no need to put comma between each column specs.\n",
    "\n",
    "### Topics to be covered in the examples\n",
    "- [Column_Attributes](#column_attributes): using attributes on column spec\n",
    "- [Different Separators and File Formats](#sep_file_formats): inputting from files with different formats and separators\n",
    "- [Different Input Source Types](#input_src_type)\n",
    "- [Advance Usage Examples](#advanced_examples)\n",
    "    * [Arbitrary Column Separator](#advanced_examples): reading in files with different separators on each column\n",
    "\n",
    "### Reading in the Data\n",
    "\n",
    "Now let's combine these options together and see it in action with `aq_pp` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"state\",\"highQ\",\"highQN\",\"date\"\n",
      "\"Alabama\",339.06,1042,\"2014-01-01\"\n",
      "\"Alaska\",288.75,252,\"2014-01-01\"\n",
      "\"Arizona\",303.31,1941,\"2014-01-01\"\n",
      "\"Arkansas\",361.85000000000002,576,\"2014-01-01\"\n",
      "\"California\",248.78,12096,\"2014-01-01\"\n",
      "\"Colorado\",236.31,2161,\"2014-01-01\"\n",
      "\"Connecticut\",347.89999999999998,1294,\"2014-01-01\"\n",
      "\"Delaware\",373.18000000000001,347,\"2014-01-01\"\n",
      "\"District of Columbia\",352.25999999999999,433,\"2014-01-01\"\n",
      "\"Florida\",306.43000000000001,6506,\"2014-01-01\"\n",
      "\"Georgia\",332.20999999999998,3099,\"2014-01-01\"\n",
      "\"Hawaii\",310.95999999999998,328,\"2014-01-01\"\n",
      "\"Idaho\",276.05000000000001,315,\"2014-01-01\"\n",
      "\"Illinois\",359.74000000000001,4008,\"2014-01-01\"\n",
      "\"Indiana\",336.80000000000001,1665,\"2014-01-01\"\n",
      "\"Iowa\",371.69999999999999,697,\"2014-01-01\"\n",
      "\"Kansas\",353.50999999999999,838,\"2014-01-01\"\n",
      "\"Kentucky\",337.32999999999998,1013,\"2014-01-01\"\n",
      "\"Louisiana\",377.70999999999998,1071,\"2014-01-01\"\n"
     ]
    }
   ],
   "source": [
    "# reading in the file, and displaying it by aq_pp\n",
    "aq_pp -f,+1 data/aq_input/partial_cannabis_price.csv -d S:state F:highQ I:highQN S:date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='column_attributes'></a>\n",
    "### Column Attributes\n",
    "\n",
    "Let's try to apply some of the attributes on the column spec. \n",
    "We will capitalize the name of the states, as we input the data. We can do this by adding `up` attribute on the column spec of `state` column, like below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"state\",\"highQ\",\"highQN\",\"date\"\n",
      "\"ALABAMA\",339.10000000000002,1042,\"2014-01-01\"\n",
      "\"ALASKA\",288.80000000000001,252,\"2014-01-01\"\n",
      "\"ARIZONA\",303.30000000000001,1941,\"2014-01-01\"\n",
      "\"ARKANSAS\",361.89999999999998,576,\"2014-01-01\"\n",
      "\"CALIFORNIA\",248.80000000000001,12096,\"2014-01-01\"\n",
      "\"COLORADO\",236.30000000000001,2161,\"2014-01-01\"\n",
      "\"CONNECTICUT\",347.89999999999998,1294,\"2014-01-01\"\n",
      "\"DELAWARE\",373.19999999999999,347,\"2014-01-01\"\n",
      "\"DISTRICT OF COLUMBIA\",352.30000000000001,433,\"2014-01-01\"\n",
      "\"FLORIDA\",306.39999999999998,6506,\"2014-01-01\"\n",
      "\"GEORGIA\",332.19999999999999,3099,\"2014-01-01\"\n",
      "\"HAWAII\",311,328,\"2014-01-01\"\n",
      "\"IDAHO\",276.10000000000002,315,\"2014-01-01\"\n",
      "\"ILLINOIS\",359.69999999999999,4008,\"2014-01-01\"\n",
      "\"INDIANA\",336.80000000000001,1665,\"2014-01-01\"\n",
      "\"IOWA\",371.69999999999999,697,\"2014-01-01\"\n",
      "\"KANSAS\",353.5,838,\"2014-01-01\"\n",
      "\"KENTUCKY\",337.30000000000001,1013,\"2014-01-01\"\n",
      "\"LOUISIANA\",377.69999999999999,1071,\"2014-01-01\"\n",
      "\"MAINE\",321.10000000000002,450,\"2014-01-01\"\n"
     ]
    }
   ],
   "source": [
    "aq_pp -f,+1 data/aq_input/partial_cannabis_price.csv -d S,up:state F:highQ I:highQN S:date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also set the characters all lower, using `lo`. \n",
    "More column attributes are available to use, refer to the [documentation](http://auriq.com/documentation/source/reference/manpages/aq-input.html?highlight=input) for more details.\n",
    "\n",
    "<a id='sep_file_formats'></a>\n",
    "### Different Separators and File Formats\n",
    "\n",
    "Not all the data files comes in clean csv format (we wish...), and as a data scientist / engineer, we need to be able to handle data in all kinds of format.\n",
    "\n",
    "Let's take a look at how to input data with different format.\n",
    "\n",
    "#### TSV\n",
    "\n",
    "This is probably the second most common file format for data.\n",
    "\n",
    "Let's assume that the data from earlier was separated by tab, like this.\n",
    "\n",
    "```\n",
    "State   HighQ   HighQN  MedQ    MedQN   LowQ    LowQN   date\n",
    "Alabama 339.06  1042    198.63999999999999      933     149.49000000000001      123     2014-01-01\n",
    "Alaska  288.75  252     260.60000000000002      297     388.57999999999998      26      2014-01-01\n",
    "Arizona 303.31  1941    209.34999999999999      1625    189.44999999999999      222     2014-01-01\n",
    "Arkansas        361.85000000000002      576     185.62  544     125.87  112     2014-01-01\n",
    "California      248.78  12096   193.56  12812   192.91999999999999      778     2014-01-01\n",
    "Colorado        236.31  2161    195.28999999999999      1728    213.5   128     2014-01-01\n",
    "Connecticut     347.89999999999998      1294    273.97000000000003      1316    257.36000000000001      91      2014-01-01\n",
    "Delaware        373.18000000000001      347     226.25  273     199.88  34      2014-01-01\n",
    "District of Columbia    352.25999999999999      433     295.67000000000002      349     213.72  39      2014-01-01\n",
    "```\n",
    "And the file name is `partial_cannabis_price.tsv`. \n",
    "All we have to do is add `tsv` attributes on the input spec, like below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"state\",\"highQ\",\"highQN\",\"date\"\n",
      "\"Alabama\",339.10000000000002,1042,\"2014-01-01\"\n",
      "\"Alaska\",288.80000000000001,252,\"2014-01-01\"\n",
      "\"Arizona\",303.30000000000001,1941,\"2014-01-01\"\n",
      "\"Arkansas\",361.89999999999998,576,\"2014-01-01\"\n",
      "\"California\",248.80000000000001,12096,\"2014-01-01\"\n",
      "\"Colorado\",236.30000000000001,2161,\"2014-01-01\"\n",
      "\"Connecticut\",347.89999999999998,1294,\"2014-01-01\"\n",
      "\"Delaware\",373.19999999999999,347,\"2014-01-01\"\n",
      "\"District of Columbia\",352.30000000000001,433,\"2014-01-01\"\n",
      "\"Florida\",306.39999999999998,6506,\"2014-01-01\"\n",
      "\"Georgia\",332.19999999999999,3099,\"2014-01-01\"\n",
      "\"Hawaii\",311,328,\"2014-01-01\"\n",
      "\"Idaho\",276.10000000000002,315,\"2014-01-01\"\n",
      "\"Illinois\",359.69999999999999,4008,\"2014-01-01\"\n",
      "\"Indiana\",336.80000000000001,1665,\"2014-01-01\"\n",
      "\"Iowa\",371.69999999999999,697,\"2014-01-01\"\n",
      "\"Kansas\",353.5,838,\"2014-01-01\"\n",
      "\"Kentucky\",337.30000000000001,1013,\"2014-01-01\"\n",
      "\"Louisiana\",377.69999999999999,1071,\"2014-01-01\"\n",
      "\"Maine\",321.10000000000002,450,\"2014-01-01\"\n"
     ]
    }
   ],
   "source": [
    "aq_pp -f,+1,tsv data/aq_input/partial_cannabis_price.tsv -d S:state F:highQ I:highQN S:date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the simple modification of command, we read in the tsv file!\n",
    "\n",
    "#### Using `sep`\n",
    "\n",
    "The case for tsv and csv wasn't too bad. But what if someone decided to use completely random separator character?\n",
    "No worries, we can use `sep` attributes in input spec, and assgin **any character** as a separator.(Note that the character has to be single byte character)\n",
    "\n",
    "For this example, we'll assume that the creator of the cannibas data was under influence, and used `+` as a separator instead of comma ;)\n",
    "\n",
    "The file name is `partial_cannabis_price.plus`. <br>\n",
    "Taking a look at the file with `head` command, it looks like below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State+HighQ+HighQN+date\n",
      "Alabama+339.1+1042+2014-01-01\n",
      "Alaska+288.8+252+2014-01-01\n",
      "Arizona+303.3+1941+2014-01-01\n",
      "Arkansas+361.9+576+2014-01-01\n",
      "California+248.8+12096+2014-01-01\n",
      "Colorado+236.3+2161+2014-01-01\n",
      "Connecticut+347.9+1294+2014-01-01\n",
      "Delaware+373.2+347+2014-01-01\n",
      "District of Columbia+352.3+433+2014-01-01\n"
     ]
    }
   ],
   "source": [
    "head data/aq_input/partial_cannabis_price.plus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to read in the file with aq-input, we'll set `+` as separator, `sep=\"+\"`, like below.<br>\n",
    "**NOTE:** Be sure to surround the seperator charactor with single or double quotes, when using `sep` option like below.\n",
    "- `sep=\";\"`\n",
    "- `sep=';'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"state\",\"highQ\",\"highQN\",\"date\"\n",
      "\"Alabama\",339.10000000000002,1042,\"2014-01-01\"\n",
      "\"Alaska\",288.80000000000001,252,\"2014-01-01\"\n",
      "\"Arizona\",303.30000000000001,1941,\"2014-01-01\"\n",
      "\"Arkansas\",361.89999999999998,576,\"2014-01-01\"\n",
      "\"California\",248.80000000000001,12096,\"2014-01-01\"\n",
      "\"Colorado\",236.30000000000001,2161,\"2014-01-01\"\n",
      "\"Connecticut\",347.89999999999998,1294,\"2014-01-01\"\n",
      "\"Delaware\",373.19999999999999,347,\"2014-01-01\"\n",
      "\"District of Columbia\",352.30000000000001,433,\"2014-01-01\"\n",
      "\"Florida\",306.39999999999998,6506,\"2014-01-01\"\n",
      "\"Georgia\",332.19999999999999,3099,\"2014-01-01\"\n",
      "\"Hawaii\",311,328,\"2014-01-01\"\n",
      "\"Idaho\",276.10000000000002,315,\"2014-01-01\"\n",
      "\"Illinois\",359.69999999999999,4008,\"2014-01-01\"\n",
      "\"Indiana\",336.80000000000001,1665,\"2014-01-01\"\n",
      "\"Iowa\",371.69999999999999,697,\"2014-01-01\"\n",
      "\"Kansas\",353.5,838,\"2014-01-01\"\n",
      "\"Kentucky\",337.30000000000001,1013,\"2014-01-01\"\n",
      "\"Louisiana\",377.69999999999999,1071,\"2014-01-01\"\n",
      "\"Maine\",321.10000000000002,450,\"2014-01-01\"\n"
     ]
    }
   ],
   "source": [
    "aq_pp -f,+1,sep='+' data/aq_input/partial_cannabis_price.plus -d S:state F:highQ I:highQN S:date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id='input_src_type'></a>\n",
    "### Different Input Source Types\n",
    "\n",
    "As we saw eariler, aq_tools can read in data from various input data sources.\n",
    "Here we will cover \n",
    "- standard input (stdout)\n",
    "- named pipe\n",
    "\n",
    "#### Standard Input\n",
    "This option comes in very handy when you're working with other linux command / aq_commands, and want to feed the output from the command into another aq_command. \n",
    "You'd do this with linux piping.(For more details of piping and redirection, take a look [here](https://ryanstutorials.net/linuxtutorial/piping.php))\n",
    "\n",
    "In the example below we'll demonstrate it with [`cat`](https://www.geeksforgeeks.org/cat-command-in-linux-with-examples/) command to print the file contents to stdout, then pipe it into aq_pp command. \n",
    "\n",
    "\n",
    "When getting input from stdout, we provide `-` instead of fileName on input spec.  Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"state\",\"highQ\",\"highQN\",\"date\"\n",
      "\"Alabama\",339.10000000000002,1042,\"2014-01-01\"\n",
      "\"Alaska\",288.80000000000001,252,\"2014-01-01\"\n",
      "\"Arizona\",303.30000000000001,1941,\"2014-01-01\"\n",
      "\"Arkansas\",361.89999999999998,576,\"2014-01-01\"\n",
      "\"California\",248.80000000000001,12096,\"2014-01-01\"\n",
      "\"Colorado\",236.30000000000001,2161,\"2014-01-01\"\n",
      "\"Connecticut\",347.89999999999998,1294,\"2014-01-01\"\n",
      "\"Delaware\",373.19999999999999,347,\"2014-01-01\"\n",
      "\"District of Columbia\",352.30000000000001,433,\"2014-01-01\"\n",
      "\"Florida\",306.39999999999998,6506,\"2014-01-01\"\n",
      "\"Georgia\",332.19999999999999,3099,\"2014-01-01\"\n",
      "\"Hawaii\",311,328,\"2014-01-01\"\n",
      "\"Idaho\",276.10000000000002,315,\"2014-01-01\"\n",
      "\"Illinois\",359.69999999999999,4008,\"2014-01-01\"\n",
      "\"Indiana\",336.80000000000001,1665,\"2014-01-01\"\n",
      "\"Iowa\",371.69999999999999,697,\"2014-01-01\"\n",
      "\"Kansas\",353.5,838,\"2014-01-01\"\n",
      "\"Kentucky\",337.30000000000001,1013,\"2014-01-01\"\n",
      "\"Louisiana\",377.69999999999999,1071,\"2014-01-01\"\n",
      "\"Maine\",321.10000000000002,450,\"2014-01-01\"\n"
     ]
    }
   ],
   "source": [
    "cat data/aq_input/partial_cannabis_price.csv | aq_pp -f,+1 - -d S:state F:highQ I:highQN S:date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compressed Data**<br>\n",
    "\n",
    "You can use `-c` option on [`gunzip`](https://www.geeksforgeeks.org/gunzip-command-in-linux-with-examples/) command, which will stream the content of the compressed file without making permanent change to the original file itself. And the stream can be inputted into aq_tools, with piping option like above example.\n",
    "\n",
    "Here we'll use the compressed version of `partial_cannabis_price.csv`, which look like the very first data we used, if uncompressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"state\",\"highQ\",\"highQN\",\"date\"\n",
      "\"Alabama\",339.10000000000002,1042,\"2014-01-01\"\n",
      "\"Alaska\",288.80000000000001,252,\"2014-01-01\"\n",
      "\"Arizona\",303.30000000000001,1941,\"2014-01-01\"\n",
      "\"Arkansas\",361.89999999999998,576,\"2014-01-01\"\n",
      "\"California\",248.80000000000001,12096,\"2014-01-01\"\n",
      "\"Colorado\",236.30000000000001,2161,\"2014-01-01\"\n",
      "\"Connecticut\",347.89999999999998,1294,\"2014-01-01\"\n",
      "\"Delaware\",373.19999999999999,347,\"2014-01-01\"\n",
      "\"District of Columbia\",352.30000000000001,433,\"2014-01-01\"\n",
      "\"Florida\",306.39999999999998,6506,\"2014-01-01\"\n",
      "\"Georgia\",332.19999999999999,3099,\"2014-01-01\"\n",
      "\"Hawaii\",311,328,\"2014-01-01\"\n",
      "\"Idaho\",276.10000000000002,315,\"2014-01-01\"\n",
      "\"Illinois\",359.69999999999999,4008,\"2014-01-01\"\n",
      "\"Indiana\",336.80000000000001,1665,\"2014-01-01\"\n",
      "\"Iowa\",371.69999999999999,697,\"2014-01-01\"\n",
      "\"Kansas\",353.5,838,\"2014-01-01\"\n",
      "\"Kentucky\",337.30000000000001,1013,\"2014-01-01\"\n",
      "\"Louisiana\",377.69999999999999,1071,\"2014-01-01\"\n",
      "\"Maine\",321.10000000000002,450,\"2014-01-01\"\n"
     ]
    }
   ],
   "source": [
    "gunzip -c data/aq_input/partial_cannabis_price.csv.gz | aq_pp -f,+1 - -d S:state F:highQ I:highQN S:date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named Pipe\n",
    "\n",
    "Named pipe is useful to have multiple commands communicate and exchange data with each other, and aq-input supports this as input as well. For more details of named pipe, refer [this great website](https://www.linuxjournal.com/article/2156).<br>\n",
    "\n",
    "Given that you have already created the named pipe to use, you can configure aq-input like below.\n",
    "\n",
    "```... -f,+1 fifo@FileName -d ...```\n",
    "where `fileName` is path to the pipe, including the pipe's name. \n",
    "\n",
    "Let's take a look at the application. \n",
    "1. We'll start with creating named pipe called `pipy` with command `mkfifo`, in current directotry.\n",
    "2. Then we'll use `cat` command to output the content of `partial_cannabis_price.csv` which is in `data/aq-input/` directory, into the named pipe.\n",
    "3. Finally we'll read the input from the pipe using `aq_pp`\n",
    "\n",
    "Now named pipe does not work in this jupyter environment, so we'll just show you the command.\n",
    "In practice, you can open up 2 virtual terminals, TERMINAL 1 and 2, in which"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n"
     ]
    }
   ],
   "source": [
    "# TERMINAL 1\n",
    "# Make pipe and feed the output to the pipe\n",
    "\n",
    "mkfifo pipy\n",
    "\n",
    "cat data/aq_input/partial_cannabis_price.csv > pipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"state\",\"highQ\",\"highQN\",\"date\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TERMINAL 2\n",
    "# get the stream input from the pipe\n",
    "aq_pp -f,+1 fifo@pipy -d S:state F:highQ I:highQN S:date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will comes in handy, when working with multiple file inputs across different terminals, and you'd like to process all of them in one aq_command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='advanced_examples'></a>\n",
    "## Advanced Usage Example\n",
    "\n",
    "\n",
    "### Arbiturary Column Separator\n",
    "\n",
    "#### Basics \n",
    "\n",
    "We can read in data that contains different separator character(s) for each column. Let's try it with the [same dataset](#data), but this time we'll be interpretting the date column as 3 distinct string columns, as year, month and date.\n",
    "\n",
    "You can achieve this in 2 steps,\n",
    "1. using `div` attribute in input spec\n",
    "2. specify the location and character to be used as separator in column spec, using `sep` \n",
    "\n",
    "**1st step:**<br>\n",
    "Using `div` attribute to let the command know that we'll be using distinct separators for each column.\n",
    "\n",
    "`..-f,+1,div fileName ...`\n",
    "\n",
    "Remember that this option is not compatible with other attributes, such as csv or tsv.(these option will assume that file uses one kind of separator across all columns in the file.)\n",
    "\n",
    "**2nd step:**<br>\n",
    "In column spec, you can simply place `sep:'sep_character'` block in between the columns where the separator character is located in the actual file. Note that when using `div` option, seperators for all of the columns need to be specified.\n",
    "\n",
    "In our case, one of the rows will look like this.\n",
    "\n",
    "`Alabama,339.06,1042,2014-01-01`\n",
    "\n",
    "Therefore, our new column spec will look like this.\n",
    "\n",
    "`-d S:state sep:',' F:highQ sep:',' I:highQN sep:',' S:year sep:'-' S:month sep:'-' S:day`\n",
    "\n",
    "As you can see, we are using separator `-` for the new 3 columns, with `sep` option. \n",
    "For the rest, we are simply using `,` as our separator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"state\",\"highQ\",\"highQN\",\"year\",\"month\",\"day\"\n",
      "\"Alabama\",339.10000000000002,1042,\"2014\",\"01\",\"01\"\n",
      "\"Alaska\",288.80000000000001,252,\"2014\",\"01\",\"01\"\n",
      "\"Arizona\",303.30000000000001,1941,\"2014\",\"01\",\"01\"\n",
      "\"Arkansas\",361.89999999999998,576,\"2014\",\"01\",\"01\"\n",
      "\"California\",248.80000000000001,12096,\"2014\",\"01\",\"01\"\n",
      "\"Colorado\",236.30000000000001,2161,\"2014\",\"01\",\"01\"\n",
      "\"Connecticut\",347.89999999999998,1294,\"2014\",\"01\",\"01\"\n",
      "\"Delaware\",373.19999999999999,347,\"2014\",\"01\",\"01\"\n",
      "\"District of Columbia\",352.30000000000001,433,\"2014\",\"01\",\"01\"\n",
      "\"Florida\",306.39999999999998,6506,\"2014\",\"01\",\"01\"\n",
      "\"Georgia\",332.19999999999999,3099,\"2014\",\"01\",\"01\"\n",
      "\"Hawaii\",311,328,\"2014\",\"01\",\"01\"\n",
      "\"Idaho\",276.10000000000002,315,\"2014\",\"01\",\"01\"\n",
      "\"Illinois\",359.69999999999999,4008,\"2014\",\"01\",\"01\"\n",
      "\"Indiana\",336.80000000000001,1665,\"2014\",\"01\",\"01\"\n",
      "\"Iowa\",371.69999999999999,697,\"2014\",\"01\",\"01\"\n",
      "\"Kansas\",353.5,838,\"2014\",\"01\",\"01\"\n",
      "\"Kentucky\",337.30000000000001,1013,\"2014\",\"01\",\"01\"\n",
      "\"Louisiana\",377.69999999999999,1071,\"2014\",\"01\",\"01\"\n",
      "\"Maine\",321.10000000000002,450,\"2014\",\"01\",\"01\"\n"
     ]
    }
   ],
   "source": [
    "aq_pp -f,+1,div data/aq_input/partial_cannabis_price.csv -d S:state sep:',' F:highQ sep:',' I:highQN sep:',' S:year sep:'-' S:month sep:'-' S:day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that we've successfully separated date into 3 distinct columns!\n",
    "\n",
    "Let's try something more challenging.\n",
    "\n",
    "#### Advanced `Sep` example\n",
    "\n",
    "`sep` option is also perfect for inputting large unstractured data, and organizing it into tabular form. \n",
    "\n",
    "For this example, we'll take a look at web log file, which is available in our [official github repository](https://github.com/auriq/EssentiaPublic). The log files are located under `/casestudies/apache/accesslog`.\n",
    "\n",
    "Let's take a peek of the data, only the first few columns, with head command.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188.138.118.184 - - [02/Nov/2014:03:34:43 -0800] \"GET / HTTP/1.1\" 200 30003 \"-\" \"Pingdom.com_bot_version_1.4_(http://www.pingdom.com)\"\n",
      "46.23.67.107 - - [02/Nov/2014:03:34:47 -0800] \"GET / HTTP/1.0\" 301 - \"-\" \"Mozilla/5.0 (compatible; monitis - premium monitoring service; http://www.monitis.com)\"\n",
      "54.248.98.72 - - [02/Nov/2014:03:34:50 -0800] \"GET / HTTP/1.0\" 301 - \"-\" \"Mozilla/5.0 (compatible; monitis - premium monitoring service; http://www.monitis.com)\"\n",
      "91.103.66.203 - - [02/Nov/2014:03:34:59 -0800] \"GET /wp-content/plugins/contact-form-7/includes/js/scripts.js?ver=3.8.1 HTTP/1.1\" 304 - \"-\" \"Mozilla/4.0 (compatible;)\"\n",
      "173.193.219.173 - - [02/Nov/2014:03:35:12 -0800] \"GET / HTTP/1.0\" 301 - \"-\" \"Mozilla/5.0 (compatible; monitis - premium monitoring service; http://www.monitis.com)\"\n"
     ]
    }
   ],
   "source": [
    "head -n 5 data/aq_input/125-access_log-20141109"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this data is not structured into table, which makes it hard for us to perform analysis, since it is in [common log format](https://en.wikipedia.org/wiki/Common_Log_Format).\n",
    "Take a look at the link, to get the basic idea of the web log format and its data we're trying to extract. \n",
    "\n",
    "Let's extract the following data from this, and display them in tabular format.\n",
    "\n",
    "* IP address: we can use dedicated datatype IP for this field\n",
    "* date, time and timezone of the server: string\n",
    "* Request: string\n",
    "* Server's response: int\n",
    "* size of the object returned: int\n",
    "* browser: string\n",
    "\n",
    "**Note**: empty field are filled in with `-`, what to do with input spec??\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"ip\",\"datetime\",\"request\",\"return_code\",\"obj_size\",\"browser\"\n",
      "188.138.118.184,\"02/Nov/2014:03:34:43 -0800\",\"GET / HTTP/1.1\",200,30003,\"\"\"Pingdom.com_bot_version_1.4_(http://www.pingdom.com)\"\"\"\n",
      "46.23.67.107,\"02/Nov/2014:03:34:47 -0800\",\"GET / HTTP/1.0\",301,0,\"\"\"Mozilla/5.0 (compatible; monitis - premium monitoring service; http://www.monitis.com)\"\"\"\n",
      "54.248.98.72,\"02/Nov/2014:03:34:50 -0800\",\"GET / HTTP/1.0\",301,0,\"\"\"Mozilla/5.0 (compatible; monitis - premium monitoring service; http://www.monitis.com)\"\"\"\n",
      "91.103.66.203,\"02/Nov/2014:03:34:59 -0800\",\"GET /wp-content/plugins/contact-form-7/includes/js/scripts.js?ver=3.8.1 HTTP/1.1\",304,0,\"\"\"Mozilla/4.0 (compatible;)\"\"\"\n",
      "173.193.219.173,\"02/Nov/2014:03:35:12 -0800\",\"GET / HTTP/1.0\",301,0,\"\"\"Mozilla/5.0 (compatible; monitis - premium monitoring service; http://www.monitis.com)\"\"\"\n",
      "89.248.170.202,\"02/Nov/2014:03:35:20 -0800\",\"POST /xmlrpc.php HTTP/1.0\",200,370,\"\"\"Mozilla/4.0 (compatible: MSIE 7.0; Windows NT 6.0)\"\"\"\n",
      "89.248.170.202,\"02/Nov/2014:03:35:22 -0800\",\"POST /xmlrpc.php HTTP/1.0\",200,370,\"\"\"Mozilla/4.0 (compatible: MSIE 7.0; Windows NT 6.0)\"\"\"\n",
      "89.248.170.202,\"02/Nov/2014:03:35:30 -0800\",\"POST /xmlrpc.php HTTP/1.0\",200,370,\"\"\"Mozilla/4.0 (compatible: MSIE 7.0; Windows NT 6.0)\"\"\"\n",
      "50.23.94.74,\"02/Nov/2014:03:35:43 -0800\",\"GET / HTTP/1.1\",200,30003,\"\"\"Pingdom.com_bot_version_1.4_(http://www.pingdom.com)\"\"\"\n",
      "46.23.67.107,\"02/Nov/2014:03:35:47 -0800\",\"GET / HTTP/1.0\",301,0,\"\"\"Mozilla/5.0 (compatible; monitis - premium monitoring service; http://www.monitis.com)\"\"\"\n"
     ]
    }
   ],
   "source": [
    "# won't be using substitution to avoid complicated string\n",
    "aq_pp -f,div,eok data/aq_input/125-access_log-20141109 \\\n",
    "-d IP:ip sep:' - - [' S:datetime sep:'] \"' S:request sep:'\" ' I:return_code sep:' ' I:obj_size sep:' \"-\" ' S:browser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To dos for advanced examples\n",
    "\n",
    "Contents below will be added in near future.\n",
    "### Extracting Key and Values\n",
    "- with the beer dataset for json\n",
    "\n",
    "### Link to weblog analysis using aq_pp command\n",
    "This section will be added in the future update.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
