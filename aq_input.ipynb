{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aq_input Tips and Samples\n",
    "\n",
    "This notebook goes over aq_input's options and it's sample usages. \n",
    "Based on AQ Tools version: 2.0.1-1.\n",
    "\n",
    "\n",
    "Here, in order to observe the input data stream, we'll be showing examples of input spec with `aq_pp` command without data transformation. \n",
    "\n",
    "\n",
    "### Prerequisites\n",
    "Users are assumed to be equipped with decent knowledge of\n",
    "- bash commands\n",
    "- knowledge of `aq_pp` command. \n",
    "- input, column and output spec for aq_tools\n",
    "\n",
    "\n",
    "We'll be going over each options in the `aq_pp` command and it's use cases. Have the [aq_pp documentation](http://auriq.com/documentation/source/reference/manpages/aq-input.html) ready on the side, so you can refer to it whenever needed.\n",
    "We'll start with basic usage of each options, then dive into advanced usage.\n",
    "\n",
    "\n",
    "## Motivation\n",
    "Aq_input / input spec specifies behaviors and interpretations of provided data by aq_tools. \n",
    "Being able to input data sources into aq_tools in any ways you like, is an essentia skill for using aq_tools. \n",
    "\n",
    "\n",
    "## Major Components\n",
    "Below is the general structure of the aq-input \n",
    "\n",
    "`aq_command ... -f[,attributes] fileName [more filenames...] -d columnSpec`\n",
    "\n",
    "There are 2 major components in input specification of aq_tool. \n",
    "\n",
    "* `-f`: file input: specifies data source's type, and its format as well as input behaviors, and data input's name(file name).\n",
    "* `-d`: column specs: specifies numbers, names and data types of columns.\n",
    "\n",
    "Before getting into examples, let us go through variations of accepted attributes for both options.\n",
    "If you've already read the documentation and are familiar with these options, go ahead and skip this section and jump right in to the examples!\n",
    "\n",
    "Let's take a look at some commonly used options here.\n",
    "\n",
    "\n",
    "## `-f` Basic file reading options\n",
    "\n",
    "By this option, we can specify 3 things + some extra, such as input sorce type, input format and error handling. \n",
    "\n",
    "### input source type\n",
    "- file\n",
    "- stream from standard output (std)\n",
    "- named pipe\n",
    "- stream from connection to listener\n",
    "\n",
    "### input format (column separator) selection\n",
    "- `csv`: the default option.\n",
    "- `sep`: this option lets users specify their own separater, as long as it is [single byte character](https://www.quora.com/What-is-the-difference-between-single-byte-or-multibyte-unicode)\n",
    "- `div`: used with `sep` option in column spec, this let's us specify different separators for each columns. \n",
    "- `tab`: html table format\n",
    "- `jsn`: allows us to input json formatted files. \n",
    "- `xml`: xml formatted file input.\n",
    "- `bin`: aq_pp's original input format\n",
    "- `aq`: from another aq_tool outputtting aq. No column spec needed. \n",
    "\n",
    "\n",
    "### Error Handling\n",
    "- `eok`: Make error non critical, resulting in skipping the rows with error.\n",
    "\n",
    "### Others\n",
    "- `esc`: input '\\' character in the data as escape character.\n",
    "- `Num`: lines / byhtes to skip\n",
    "- `bz`: buffer size\n",
    "\n",
    "\n",
    "## `-d` Column specs\n",
    "\n",
    "Column spec specifies what data type will the each column in the data be interpreted by, as well as each column's name. \n",
    "Therefore, column spec has a format of \n",
    "``` bash\n",
    "-d dtype[,attributes]:colName dtype2[,attributes]:colName2 ... \n",
    "```\n",
    "\n",
    "\n",
    "### Generic Col Spec\n",
    "\n",
    "Supported data types are \n",
    "- `S`: string\n",
    "- `F`: Double precision floating point\n",
    "- `L`: unsigned integer\n",
    "- `LS`: 64 bit signed integer\n",
    "- `I`: 32 bit unsigned integer\n",
    "- `IS`: 32 bit signed integer\n",
    "- `IP`: v4/v6 address\n",
    "- `X`: placeholder for unwanted input column.\n",
    "\n",
    "And some of the attributes that might come in handy are...\n",
    "- `trm`: trim leading/trailing spaces from field value\n",
    "- `lo`, `up`: convert a string field value to lower or upper case. \n",
    "\n",
    "There's more attributes available, take a look at the documentation for further details.\n",
    "\n",
    "Now we are equipped with the basic knowledge of input specifications, let's get started on examples.\n",
    "\n",
    "\n",
    "## Data \n",
    "\n",
    "We will start with the simplest inputting of csv file, consists of numeric and string columns.\n",
    "This is a data of street price of cannabis across different states.(The data is modified for simplicity)\n",
    "\n",
    "Data looks like following, \n",
    "\n",
    "State|HighQ|HighQN|date\n",
    "---|---|---|---|\n",
    "Alabama|339.06|1042|2014-01-01\n",
    "Alaska|288.75|252|2014-01-01\n",
    "Arizona|303.31|1941|2014-01-01\n",
    "Arkansas|361.85000000000002|576|2014-01-01\n",
    "California|248.78|12096|2014-01-01\n",
    "Colorado|236.31|2161|2014-01-01\n",
    "Connecticut|347.89999999999998|1294|2014-01-01\n",
    "Delaware|373.18000000000001|347|2014-01-01\n",
    "District of Columbia|352.25999999999999|433|2014-01-01\n",
    "Florida|306.43000000000001|6506|2014-01-01\n",
    "\n",
    "**Actual file looks like**<br>\n",
    "```\n",
    "State,HighQ,HighQN,MedQ,MedQN,LowQ,LowQN,date\n",
    "Alabama,339.06,1042,198.63999999999999,933,149.49000000000001,123,2014-01-01\n",
    "Alaska,288.75,252,260.60000000000002,297,388.57999999999998,26,2014-01-01\n",
    "Arizona,303.31,1941,209.34999999999999,1625,189.44999999999999,222,2014-01-01\n",
    "Arkansas,361.85000000000002,576,185.62,544,125.87,112,2014-01-01\n",
    "California,248.78,12096,193.56,12812,192.91999999999999,778,2014-01-01\n",
    "Colorado,236.31,2161,195.28999999999999,1728,213.5,128,2014-01-01\n",
    "Connecticut,347.89999999999998,1294,273.97000000000003,1316,257.36000000000001,91,2014-01-01\n",
    "Delaware,373.18000000000001,347,226.25,273,199.88,34,2014-01-01\n",
    "District of Columbia,352.25999999999999,433,295.67000000000002,349,213.72,39,2014-01-01\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### Columns \n",
    "- 2 string columns, `State` and `date`\n",
    "- 1 float column, `HighQ` and\n",
    "- 1 int columns, `HighQN`\n",
    "\n",
    "### Data Format & Seperator\n",
    "Here, the file is in **csv format**, and each column is separated by comma.\n",
    "\n",
    "## Basic Usage Examples\n",
    "What we need, to read in this data is to specify\n",
    "- column spec\n",
    "- file name\n",
    "- attribute\n",
    "\n",
    "### Column Spec\n",
    "This needs to be in the same order as the actual columns in the data. Therefore, considering the data types we've looked at earlier, we can specify column specs with data types and column names.\n",
    "\n",
    "`-d S:state F:highQ I:highQN S:date`\n",
    "\n",
    "Note that data type's capitailzation does not matter, and no need to put comma between each column specs.\n",
    "\n",
    "### Input Spec\n",
    "With the above data, few things needs to be considered for input spec.\n",
    "1. attributes\n",
    "2. path to the fileName, which is `data/aq_input/partial_cannibas_price.csv`\n",
    "\n",
    "`... -f,+1 data/aq_input/partial_cannibas_price.csv ...`\n",
    "\n",
    "For attributes, we are specifying that we'll skip the first row of the file because it is a header, by providing `+1`. Because csv format is the default for input spec, we don't have to provide it explicitly.\n",
    "\n",
    "After the attribute, we can simply provide the path to the file name.\n",
    "\n",
    "### Reading in the Data\n",
    "\n",
    "Now let's combine these options together and see it in action with `aq_pp` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"state\",\"highQ\",\"highQN\",\"date\"\n",
      "\"Alabama\",339.06,1042,\"198.63999999999999\"\n",
      "\"Alaska\",288.75,252,\"260.60000000000002\"\n",
      "\"Arizona\",303.31,1941,\"209.34999999999999\"\n",
      "\"Arkansas\",361.85000000000002,576,\"185.62\"\n",
      "\"California\",248.78,12096,\"193.56\"\n",
      "\"Colorado\",236.31,2161,\"195.28999999999999\"\n",
      "\"Connecticut\",347.89999999999998,1294,\"273.97000000000003\"\n",
      "\"Delaware\",373.18000000000001,347,\"226.25\"\n",
      "\"District of Columbia\",352.25999999999999,433,\"295.67000000000002\"\n",
      "\"Florida\",306.43000000000001,6506,\"220.03\"\n",
      "\"Georgia\",332.20999999999998,3099,\"213.52000000000001\"\n",
      "\"Hawaii\",310.95999999999998,328,\"270.38\"\n",
      "\"Idaho\",276.05000000000001,315,\"254.96000000000001\"\n",
      "\"Illinois\",359.74000000000001,4008,\"287.23000000000002\"\n",
      "\"Indiana\",336.80000000000001,1665,\"206.24000000000001\"\n",
      "\"Iowa\",371.69999999999999,697,\"292.33999999999997\"\n",
      "\"Kansas\",353.50999999999999,838,\"260.97000000000003\"\n",
      "\"Kentucky\",337.32999999999998,1013,\"183.16999999999999\"\n",
      "\"Louisiana\",377.70999999999998,1071,\"243.25999999999999\"\n"
     ]
    }
   ],
   "source": [
    "# reading in the file, and displaying it by aq_pp\n",
    "aq_pp -f,+1 data/aq_input/partial_cannabis_price.csv -d S:state F:highQ I:highQN S:date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Attributes\n",
    "\n",
    "Let's try to apply some of the attributes on the column spec. \n",
    "We will capitalize the name of the states, as we input the data. We can do this by adding `up` attribute on the column spec of `state` column, like below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"state\",\"highQ\",\"highQN\",\"date\"\n",
      "\"ALABAMA\",339.06,1042,\"198.63999999999999\"\n",
      "\"ALASKA\",288.75,252,\"260.60000000000002\"\n",
      "\"ARIZONA\",303.31,1941,\"209.34999999999999\"\n",
      "\"ARKANSAS\",361.85000000000002,576,\"185.62\"\n",
      "\"CALIFORNIA\",248.78,12096,\"193.56\"\n",
      "\"COLORADO\",236.31,2161,\"195.28999999999999\"\n",
      "\"CONNECTICUT\",347.89999999999998,1294,\"273.97000000000003\"\n",
      "\"DELAWARE\",373.18000000000001,347,\"226.25\"\n",
      "\"DISTRICT OF COLUMBIA\",352.25999999999999,433,\"295.67000000000002\"\n",
      "\"FLORIDA\",306.43000000000001,6506,\"220.03\"\n",
      "\"GEORGIA\",332.20999999999998,3099,\"213.52000000000001\"\n",
      "\"HAWAII\",310.95999999999998,328,\"270.38\"\n",
      "\"IDAHO\",276.05000000000001,315,\"254.96000000000001\"\n",
      "\"ILLINOIS\",359.74000000000001,4008,\"287.23000000000002\"\n",
      "\"INDIANA\",336.80000000000001,1665,\"206.24000000000001\"\n",
      "\"IOWA\",371.69999999999999,697,\"292.33999999999997\"\n",
      "\"KANSAS\",353.50999999999999,838,\"260.97000000000003\"\n",
      "\"KENTUCKY\",337.32999999999998,1013,\"183.16999999999999\"\n",
      "\"LOUISIANA\",377.70999999999998,1071,\"243.25999999999999\"\n"
     ]
    }
   ],
   "source": [
    "aq_pp -f,+1 data/aq_input/partial_cannabis_price.csv -d S,up:state F:highQ I:highQN S:date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also set the characters all lower, using `lo`. \n",
    "More column attributes are available to use, refer to the [documentation](http://auriq.com/documentation/source/reference/manpages/aq-input.html?highlight=input) for more details.\n",
    "\n",
    "### Different Separators and File Formats\n",
    "\n",
    "Not all the data files comes in clean csv format (we wish...), and as a data scientist / engineer, we need to be able to handle data in all kinds of format.\n",
    "\n",
    "Let's take a look at how to input data with different format.\n",
    "\n",
    "#### TSV\n",
    "\n",
    "This is probably the second most common file format for data.\n",
    "\n",
    "Let's assume that the data from earlier was separated by tab, like this.\n",
    "\n",
    "```\n",
    "State   HighQ   HighQN  MedQ    MedQN   LowQ    LowQN   date\n",
    "Alabama 339.06  1042    198.63999999999999      933     149.49000000000001      123     2014-01-01\n",
    "Alaska  288.75  252     260.60000000000002      297     388.57999999999998      26      2014-01-01\n",
    "Arizona 303.31  1941    209.34999999999999      1625    189.44999999999999      222     2014-01-01\n",
    "Arkansas        361.85000000000002      576     185.62  544     125.87  112     2014-01-01\n",
    "California      248.78  12096   193.56  12812   192.91999999999999      778     2014-01-01\n",
    "Colorado        236.31  2161    195.28999999999999      1728    213.5   128     2014-01-01\n",
    "Connecticut     347.89999999999998      1294    273.97000000000003      1316    257.36000000000001      91      2014-01-01\n",
    "Delaware        373.18000000000001      347     226.25  273     199.88  34      2014-01-01\n",
    "District of Columbia    352.25999999999999      433     295.67000000000002      349     213.72  39      2014-01-01\n",
    "```\n",
    "And the file name is `partial_cannabis_price.tsv`. \n",
    "All we have to do is add `tsv` attributes on the input spec, like below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"state\",\"highQ\",\"highQN\",\"date\"\n",
      "\"Alabama\",339.06,1042,\"198.63999999999999\"\n",
      "\"Alaska\",288.75,252,\"260.60000000000002\"\n",
      "\"Arizona\",303.31,1941,\"209.34999999999999\"\n",
      "\"Arkansas\",361.85000000000002,576,\"185.62\"\n",
      "\"California\",248.78,12096,\"193.56\"\n",
      "\"Colorado\",236.31,2161,\"195.28999999999999\"\n",
      "\"Connecticut\",347.89999999999998,1294,\"273.97000000000003\"\n",
      "\"Delaware\",373.18000000000001,347,\"226.25\"\n",
      "\"District of Columbia\",352.25999999999999,433,\"295.67000000000002\"\n",
      "\"Florida\",306.43000000000001,6506,\"220.03\"\n",
      "\"Georgia\",332.20999999999998,3099,\"213.52000000000001\"\n",
      "\"Hawaii\",310.95999999999998,328,\"270.38\"\n",
      "\"Idaho\",276.05000000000001,315,\"254.96000000000001\"\n",
      "\"Illinois\",359.74000000000001,4008,\"287.23000000000002\"\n",
      "\"Indiana\",336.80000000000001,1665,\"206.24000000000001\"\n",
      "\"Iowa\",371.69999999999999,697,\"292.33999999999997\"\n",
      "\"Kansas\",353.50999999999999,838,\"260.97000000000003\"\n",
      "\"Kentucky\",337.32999999999998,1013,\"183.16999999999999\"\n",
      "\"Louisiana\",377.70999999999998,1071,\"243.25999999999999\"\n"
     ]
    }
   ],
   "source": [
    "aq_pp -f,+1,tsv data/aq_input/partial_cannabis_price.tsv -d S:state F:highQ I:highQN S:date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the simple modification of command, we read in the tsv file!\n",
    "\n",
    "#### Using `sep`\n",
    "\n",
    "The case for tsv and csv wasn't too bad. But what if someone decided to use completely random separator character?\n",
    "No worries, we can use `sep` attributes in input spec, and assgin **any character** as a separator.(Note that the character has to be single byte character)\n",
    "\n",
    "For this example, we'll assume that the creator of the cannibas data was under influence, and used `+` as a separator instead of comma, which look like below.\n",
    "\n",
    "```\n",
    "State+HighQ+HighQN+MedQ+MedQN+LowQ+LowQN+date\n",
    "Alabama+339.06+1042+198.63999999999999+933+149.49000000000001+123+2014-01-01\n",
    "Alaska+288.75+252+260.60000000000002+297+388.57999999999998+26+2014-01-01\n",
    "Arizona+303.31+1941+209.34999999999999+1625+189.44999999999999+222+2014-01-01\n",
    "Arkansas+361.85000000000002+576+185.62+544+125.87+112+2014-01-01\n",
    "California+248.78+12096+193.56+12812+192.91999999999999+778+2014-01-01\n",
    "Colorado+236.31+2161+195.28999999999999+1728+213.5+128+2014-01-01\n",
    "Connecticut+347.89999999999998+1294+273.97000000000003+1316+257.36000000000001+91+2014-01-01\n",
    "Delaware+373.18000000000001+347+226.25+273+199.88+34+2014-01-01\n",
    "District of Columbia+352.25999999999999+433+295.67000000000002+349+213.72+39+2014-01-01\n",
    "```\n",
    "\n",
    "The file name is `partial_cannabis_price.plus`. <br>\n",
    "We'll set `+` as separator, `sep=\"+\"`, like below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"state\",\"highQ\",\"highQN\",\"date\"\n",
      "\"Alabama\",339.06,1042,\"198.63999999999999\"\n",
      "\"Alaska\",288.75,252,\"260.60000000000002\"\n",
      "\"Arizona\",303.31,1941,\"209.34999999999999\"\n",
      "\"Arkansas\",361.85000000000002,576,\"185.62\"\n",
      "\"California\",248.78,12096,\"193.56\"\n",
      "\"Colorado\",236.31,2161,\"195.28999999999999\"\n",
      "\"Connecticut\",347.89999999999998,1294,\"273.97000000000003\"\n",
      "\"Delaware\",373.18000000000001,347,\"226.25\"\n",
      "\"District of Columbia\",352.25999999999999,433,\"295.67000000000002\"\n",
      "\"Florida\",306.43000000000001,6506,\"220.03\"\n",
      "\"Georgia\",332.20999999999998,3099,\"213.52000000000001\"\n",
      "\"Hawaii\",310.95999999999998,328,\"270.38\"\n",
      "\"Idaho\",276.05000000000001,315,\"254.96000000000001\"\n",
      "\"Illinois\",359.74000000000001,4008,\"287.23000000000002\"\n",
      "\"Indiana\",336.80000000000001,1665,\"206.24000000000001\"\n",
      "\"Iowa\",371.69999999999999,697,\"292.33999999999997\"\n",
      "\"Kansas\",353.50999999999999,838,\"260.97000000000003\"\n",
      "\"Kentucky\",337.32999999999998,1013,\"183.16999999999999\"\n",
      "\"Louisiana\",377.70999999999998,1071,\"243.25999999999999\"\n"
     ]
    }
   ],
   "source": [
    "aq_pp -f,+1,sep='+' data/aq_input/partial_cannabis_price.plus -d S:state F:highQ I:highQN S:date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Be sure to surround the seperator charactor with single or double quotes, when using `sep` option like below.\n",
    "- `sep=\";\"`\n",
    "- `sep=';'`\n",
    "\n",
    "### Input Source Type\n",
    "\n",
    "As we saw eariler, aq_tools can read in data from various input data sources.\n",
    "Here we will cover \n",
    "- standard input (stdout)\n",
    "- named pipe\n",
    "\n",
    "#### Standard Input\n",
    "This option comes in very handy when you're working with other linux command / aq_commands, and want to feed the output from the command into another aq_tool. \n",
    "You'd do this with linux piping.(For more details of piping and redirection, take a look [here](https://ryanstutorials.net/linuxtutorial/piping.php))\n",
    "\n",
    "For here, we'll demonstrate it with [`cat`](https://www.geeksforgeeks.org/cat-command-in-linux-with-examples/) command to print the file contents to stdout, then pipe it into aq_pp command. \n",
    "\n",
    "\n",
    "When getting input from stdout, we provide `-` instead of fileName on input spec. Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"state\",\"highQ\",\"highQN\",\"date\"\n",
      "\"Alabama\",339.06,1042,\"198.63999999999999\"\n",
      "\"Alaska\",288.75,252,\"260.60000000000002\"\n",
      "\"Arizona\",303.31,1941,\"209.34999999999999\"\n",
      "\"Arkansas\",361.85000000000002,576,\"185.62\"\n",
      "\"California\",248.78,12096,\"193.56\"\n",
      "\"Colorado\",236.31,2161,\"195.28999999999999\"\n",
      "\"Connecticut\",347.89999999999998,1294,\"273.97000000000003\"\n",
      "\"Delaware\",373.18000000000001,347,\"226.25\"\n",
      "\"District of Columbia\",352.25999999999999,433,\"295.67000000000002\"\n",
      "\"Florida\",306.43000000000001,6506,\"220.03\"\n",
      "\"Georgia\",332.20999999999998,3099,\"213.52000000000001\"\n",
      "\"Hawaii\",310.95999999999998,328,\"270.38\"\n",
      "\"Idaho\",276.05000000000001,315,\"254.96000000000001\"\n",
      "\"Illinois\",359.74000000000001,4008,\"287.23000000000002\"\n",
      "\"Indiana\",336.80000000000001,1665,\"206.24000000000001\"\n",
      "\"Iowa\",371.69999999999999,697,\"292.33999999999997\"\n",
      "\"Kansas\",353.50999999999999,838,\"260.97000000000003\"\n",
      "\"Kentucky\",337.32999999999998,1013,\"183.16999999999999\"\n",
      "\"Louisiana\",377.70999999999998,1071,\"243.25999999999999\"\n"
     ]
    }
   ],
   "source": [
    "cat data/aq_input/partial_cannabis_price.csv | aq_pp -f,+1 - -d S:state F:highQ I:highQN S:date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus Question:** <br>\n",
    "How would you process a compressed csv data file (`fileName.gz`) using pipe?<br>\n",
    "HINT:\n",
    "* use `gunzip` command with pipeing\n",
    "* you don't actually have to create an uncompressed copy of the datafile\n",
    "\n",
    "Did you think about it..??\n",
    "\n",
    "Here is the answer.\n",
    "\n",
    "You can use `-c` option on gunzip, which will stream the content of the compressed file without making permanent change to the original file itself. And the stream can be inputted into aq_tools, with piping option.\n",
    "\n",
    "Here we'll use the compressed version of `partial_cannabis_price.csv`, which look like the very first data we used, if uncompressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"state\",\"highQ\",\"highQN\",\"date\"\n",
      "\"Alabama\",339.06,1042,\"198.63999999999999\"\n",
      "\"Alaska\",288.75,252,\"260.60000000000002\"\n",
      "\"Arizona\",303.31,1941,\"209.34999999999999\"\n",
      "\"Arkansas\",361.85000000000002,576,\"185.62\"\n",
      "\"California\",248.78,12096,\"193.56\"\n",
      "\"Colorado\",236.31,2161,\"195.28999999999999\"\n",
      "\"Connecticut\",347.89999999999998,1294,\"273.97000000000003\"\n",
      "\"Delaware\",373.18000000000001,347,\"226.25\"\n",
      "\"District of Columbia\",352.25999999999999,433,\"295.67000000000002\"\n",
      "\"Florida\",306.43000000000001,6506,\"220.03\"\n",
      "\"Georgia\",332.20999999999998,3099,\"213.52000000000001\"\n",
      "\"Hawaii\",310.95999999999998,328,\"270.38\"\n",
      "\"Idaho\",276.05000000000001,315,\"254.96000000000001\"\n",
      "\"Illinois\",359.74000000000001,4008,\"287.23000000000002\"\n",
      "\"Indiana\",336.80000000000001,1665,\"206.24000000000001\"\n",
      "\"Iowa\",371.69999999999999,697,\"292.33999999999997\"\n",
      "\"Kansas\",353.50999999999999,838,\"260.97000000000003\"\n",
      "\"Kentucky\",337.32999999999998,1013,\"183.16999999999999\"\n",
      "\"Louisiana\",377.70999999999998,1071,\"243.25999999999999\"\n"
     ]
    }
   ],
   "source": [
    "gunzip -c data/aq_input/partial_cannabis_price.gz | aq_pp -f,+1 - -d S:state F:highQ I:highQN S:date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named Pipe\n",
    "\n",
    "For named pipe, once you've created the named pipe and arranged the input, you can pass `fifo@FileName` to input spec, like this.\n",
    "\n",
    "```... -f,+1 fifo@FileName -d ...```\n",
    "where `fileName` is path to the pipe, including the pipe's name. \n",
    "For more details of named pipe, refer [this great website](https://www.linuxjournal.com/article/2156)\n",
    "\n",
    "Let's take a look at the application. \n",
    "1. We'll start with creating named pipe called `pipy` with command `mkfifo`, in current directotry.\n",
    "2. Then we'll use `cat` command to output the content of `partial_cannabis_price.csv` which is in `data/aq-input/` directory, into the named pipe.\n",
    "3. Finally we'll read the input from the pipe using `aq_pp`\n",
    "\n",
    "Now named pipe does not work in this jupyter environment, so we'll just show you the command.\n",
    "In practice, you can open up 2 virtual terminals, in which"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n"
     ]
    }
   ],
   "source": [
    "# TERMINAL 1\n",
    "# Make pipe and feed the output to the pipe\n",
    "\n",
    "mkfifo pipy\n",
    "\n",
    "cat data/aq_input/partial_cannabis_price.csv > pipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"state\",\"highQ\",\"highQN\",\"date\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TERMINAL 2\n",
    "# get the stream input from the pipe\n",
    "aq_pp -f,+1 fifo@pipy -d S:state F:highQ I:highQN S:date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will comes in handy, when working with multiple file inputs across different terminals, and you'd like to process all of them in one aq_command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Usage Example\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Arbiturary Column Separator\n",
    "\n",
    "#### Basics \n",
    "\n",
    "We can set up different separating characters for each columns. Let's try it with the same dataset, but this time we'll be importing the date as 3 distinct string columns, as year, month and date.\n",
    "\n",
    "First thing we need to do is to let the command know that we'll be using distinct separators. We do this in input spec, like\n",
    "\n",
    "`..-f,+1,div fileName ...`\n",
    "\n",
    "where `div` tells the command that we'll use our own and different separator for each column. Remember that this option is not compatible with other attributes, such as csv or tsv.(these option will assume that file uses one kind of separator across all columns in the file.)\n",
    "\n",
    "Now we will actually specify the separator for each columns. You can simply place `sep:'sep_character'` block in between the columns where the separator character will be located in the file. Note that when using `div` option, seperators for all of the columns need to be specified.\n",
    "\n",
    "In our case, one of the row will look like this.\n",
    "\n",
    "`Alabama,339.06,1042,2014-01-01`\n",
    "\n",
    "Therefore, our new column spec will look like this.\n",
    "\n",
    "`-d S:state sep:',' F:highQ sep:',' I:highQN sep:',' S:year sep:'-' S:month sep:'-' S:day`\n",
    "\n",
    "As you can see, we are using separator `-` for the new 3 columns, with `sep` option. \n",
    "For the rest, we are simply using `,` as our separator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"state\",\"highQ\",\"highQN\",\"year\",\"month\",\"day\"\n",
      "data/aq_input/partial_cannabis_price.csv: Unexpected end of file: byte=1+861 rec=1 field=#7\n",
      "aq_pp: Input processing error\n"
     ]
    },
    {
     "ename": "",
     "evalue": "13",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "aq_pp -f,+1,div data/aq_input/partial_cannabis_price.csv -d  S:state sep:',' F:highQ sep:',' I:highQN sep:',' S:year sep:'-' S:month sep:'-' S:day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that we've successfully separated date into 3 distinct columns!\n",
    "\n",
    "Let's try something more challenging.\n",
    "\n",
    "#### Advanced `Sep` example\n",
    "\n",
    "`sep` option is also perfect for inputting large unstractured data, and organizing it into tabular form. \n",
    "\n",
    "For this example, we'll take a look at web log file, which is available in our [official github repository](https://github.com/auriq/EssentiaPublic). The log files are located under `/casestudies/apache/accesslog`.\n",
    "\n",
    "Let's take a peek of the data, only the first few columns, with head command.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188.138.118.184 - - [02/Nov/2014:03:34:43 -0800] \"GET / HTTP/1.1\" 200 30003 \"-\" \"Pingdom.com_bot_version_1.4_(http://www.pingdom.com)\"\n",
      "46.23.67.107 - - [02/Nov/2014:03:34:47 -0800] \"GET / HTTP/1.0\" 301 - \"-\" \"Mozilla/5.0 (compatible; monitis - premium monitoring service; http://www.monitis.com)\"\n",
      "54.248.98.72 - - [02/Nov/2014:03:34:50 -0800] \"GET / HTTP/1.0\" 301 - \"-\" \"Mozilla/5.0 (compatible; monitis - premium monitoring service; http://www.monitis.com)\"\n"
     ]
    }
   ],
   "source": [
    "head -n 3 data/aq_input/125-access_log-20141109"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this data is not structured into table, which makes it hard for us to perform analysis, since it is in [common log format](https://en.wikipedia.org/wiki/Common_Log_Format).\n",
    "Take a look at the link, to get the basic idea of the web log format and its data we're trying to extract. \n",
    "\n",
    "Let's extract the following data from this, and display them in tabular format.\n",
    "\n",
    "* IP address: we can use dedicated datatype IP for this field\n",
    "* date, time and timezone of the server: string\n",
    "* Request: string\n",
    "* Server's response: int\n",
    "* size of the object returned: int\n",
    "* browser: string\n",
    "\n",
    "**Note**: empty field are filled in with `-`, what to do with input spec??\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown option \"-\": ... - - [\" S:datetime ...\n",
      "aq_pp: Option spec error\n"
     ]
    },
    {
     "ename": "",
     "evalue": "2",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "# first set up col spec as variable, since it's super long\n",
    "cols='IP:ip sep:\" - - [\" S:datetime sep:\"] \\\"\" S:request sep=\"\\\" \" I:obj_size sep:\" \\\"-\\\" \" S:browser'\n",
    "aq_pp -f,div,eok data/aq_input/125-access_log-20141109 -d $cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To dos for advanced examples\n",
    "\n",
    "\n",
    "### Extracting Key and Values\n",
    "- with the beer dataset for json\n",
    "\n",
    "This section will be added in the future update.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
