{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to README](../../README.ipynb)\n",
    "\n",
    "# Web Data Anlaysis\n",
    "\n",
    "## Data Source \n",
    "\n",
    "- raw web log data available as a part of [this dataset](http://old.honeynet.org/scans/scan34/) is used for this notebook.\n",
    "- the weblogs stored at s3 bucket `/essentia-playground/Weblog_Dataset/http/`\n",
    "\n",
    "Here, I'm planning on demonstrating \n",
    "1. deviding the log data into columns (converting to table structure)\n",
    "2. perform some EDAs and visualization\n",
    "3. pick features for anomaly detection \n",
    "4. format the data for suitable use for Sagemaker Random Cut Forest (RCF) algorithm.\n",
    "\n",
    "Then put the data back to different S3 bucket. Step 3 and 4 are not inconclusive though, depends on the result from step 2.\n",
    "\n",
    "## Setup\n",
    "First we'll start with selecting the bucket as our datastore, then creating a category called `weblogs` and include all the raw apache web log data files in the bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-27 22:23:32 ip-10-10-1-118 ess[3121]: Fetching file list from datastore.\n",
      "2019-11-27 22:23:32 ip-10-10-1-118 ess[3121]: Examining largest matched file to determine compression type: /Weblog_Dataset/http/access_log.1\n",
      "2019-11-27 22:23:32 ip-10-10-1-118 ess[3121]: Probing largest matched file to determine data configuration: /Weblog_Dataset/http/access_log.1\n",
      "Name:        weblogs\n",
      "Pattern:     /Weblog_Dataset/http/access_log.*\n",
      "Exclude:     None\n",
      "Date Format: auto\n",
      "Date Regex:  \n",
      "Archive:     \n",
      "Delimiter:   Space\n",
      "# of files:  6\n",
      "Total size:  439.4KB\n",
      "File range:  2019-11-01 - 2019-11-06\n",
      "# columns:   10\n",
      "Column Spec: IP:col_1 S:col_2 S:col_3 S:col_4 S:col_5 S:col_6 I:col_7 I:col_8 S:col_9 S:col_10\n",
      "Pkey: \n",
      "Schema: IP:col_1 S:col_2 S:col_3 S:col_4 S:col_5 S:col_6 I:col_7 I:col_8 S:col_9 S:col_10\n",
      "Preprocess:  \n",
      "usecache:    False\n",
      "Comment:    \n",
      "\n",
      "First few lines:\n",
      "24.196.254.170 - - [06/Mar/2005:05:28:52 -0500] \"GET / HTTP/1.1\" 403 2898 \"-\" \"Mozilla/4.0 (compatible; MSIE 5.5; Windows 98)\"\n",
      "192.195.225.6 - - [06/Mar/2005:05:31:37 -0500] \"GET / HTTP/1.1\" 403 2898 \"-\" \"Mozilla/4.0 (compatible; MSIE 5.5; Windows 98)\"\n",
      "218.75.19.178 - - [06/Mar/2005:06:37:20 -0500] \"GET / HTTP/1.1\" 403 2898 \"-\" \"Mozilla/4.0 (compatible; MSIE 5.5; Windows 98)\"\n",
      "65.115.46.225 - - [06/Mar/2005:07:45:21 -0500] \"GET / HTTP/1.1\" 403 2898 \"-\" \"Mozilla/4.0 (compatible; MSIE 5.5; Windows 98)\"\n",
      "219.137.79.110 - - [06/Mar/2005:09:05:38 -0500] \"GET / HTTP/1.1\" 403 2898 \"-\" \"Mozilla/4.0 (compatible; MSIE 5.5; Windows 98)\"\n"
     ]
    }
   ],
   "source": [
    "# select datastore, create category and take a peek at the data\n",
    "ess select s3://essentia-playground\n",
    "ess category add weblogs \"/Weblog_Dataset/http/access_log.*\"\n",
    "ess summary weblogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "It looks like each row is an entry of server access log in [combined log format](https://httpd.apache.org/docs/2.4/logs.html#combine_log), which is one long string. <br>\n",
    "This cannot be analyzed in the format, so let's break it down to detailed columns. Followings are the list of columns we can get from the file, from left to right in the row log format.\n",
    "\n",
    "```24.196.254.170 - - [06/Mar/2005:05:28:52 -0500] \"GET / HTTP/1.1\" 403 2898 \"-\" \"Mozilla/4.0 (compatible; MSIE 5.5; Windows 98)\"```\n",
    "\n",
    "* `24.196.254.170` - Client IP Address\n",
    "* `-` - hyphen indicates request piece of info is not available.\n",
    "* `-` - user id of the person who is requesting the documentsl\n",
    "* `[06/Mar/2005:05:28:52 -0500]` - date, month, year and time of the request\n",
    "* `\"GET / HTTP/1.1\"` - requet line given from client. This can be broken down to smaller columns\n",
    "* `403` - status code that the server sent back to client. \n",
    "* `2898` - size of object returned to the cilent\n",
    "* `\"-\"` - this can contain URLs or domain name, and is referer.\n",
    "* `\"Mozilla/4.0 (compatible; MSIE 5.5; Windows 98)\"` - user agent http request header. \n",
    "\n",
    "Out of all columns, request line and time columns will be separated to more detailed columns.\n",
    "\n",
    "We can do this using `aq_pp` command. But first, let us create a column specs. It looks like `ess category add` was unable to scan the column spec we wanted, so we can make it ourselves. \n",
    "Below is the column spec that will be used for the rest of the notebook.\n",
    "\n",
    "```IP:ip sep:' ' S:hypen sep:' ' S:user_id sep:' [' I:date sep:'/' S:month sep:'/' I:year sep:':' I:hour sep:':' I:minute sep:':' I:second sep:' ' S:time_dif sep:'] \"' S:request_method sep:' ' S:requested_resource sep:' ' S:protocol sep:'\" ' I:return_code sep:' ' I:obj_size sep:' ' S:referrer sep:' ' S:user_agent```\n",
    "\n",
    "**Attributes**<br>\n",
    "\n",
    "* `sep:` attributes were used alongside wtih \n",
    "* `div` to specify separate delimiters for each columns.\n",
    "* `eok` was used to skip the line which include invalid data. works as data cleaning function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"ip\",\"hypen\",\"user_id\",\"date\",\"month\",\"year\",\"hour\",\"minute\",\"second\",\"time_dif\",\"request_method\",\"requested_resource\",\"protocol\",\"return_code\",\"obj_size\",\"referrer\",\"user_agent\"\n",
      "192.195.225.6,\"-\",\"-\",6,\"Mar\",2005,5,31,37,\"-0500\",\"GET\",\"/\",\"HTTP/1.1\",403,2898,\"\"\"-\"\"\",\"\"\"Mozilla/4.0 (compatible; MSIE 5.5; Windows 98)\"\"\"\n",
      "218.75.19.178,\"-\",\"-\",6,\"Mar\",2005,6,37,20,\"-0500\",\"GET\",\"/\",\"HTTP/1.1\",403,2898,\"\"\"-\"\"\",\"\"\"Mozilla/4.0 (compatible; MSIE 5.5; Windows 98)\"\"\"\n",
      "65.115.46.225,\"-\",\"-\",6,\"Mar\",2005,7,45,21,\"-0500\",\"GET\",\"/\",\"HTTP/1.1\",403,2898,\"\"\"-\"\"\",\"\"\"Mozilla/4.0 (compatible; MSIE 5.5; Windows 98)\"\"\"\n",
      "219.137.79.110,\"-\",\"-\",6,\"Mar\",2005,9,5,38,\"-0500\",\"GET\",\"/\",\"HTTP/1.1\",403,2898,\"\"\"-\"\"\",\"\"\"Mozilla/4.0 (compatible; MSIE 5.5; Windows 98)\"\"\"\n",
      "219.61.8.78,\"-\",\"-\",6,\"Mar\",2005,9,12,31,\"-0500\",\"GET\",\"/\",\"HTTP/1.1\",403,2898,\"\"\"-\"\"\",\"\"\"Mozilla/4.0 (compatible; MSIE 5.5; Windows 98)\"\"\"\n",
      "208.57.191.112,\"-\",\"-\",6,\"Mar\",2005,10,31,42,\"-0500\",\"GET\",\"/\",\"HTTP/1.1\",403,2898,\"\"\"-\"\"\",\"\"\"Mozilla/4.0 (compatible; MSIE 5.5; Windows 98)\"\"\"\n",
      "64.90.198.61,\"-\",\"-\",6,\"Mar\",2005,12,19,50,\"-0500\",\"GET\",\"/\",\"HTTP/1.1\",403,2898,\"\"\"-\"\"\",\"\"\"Mozilla/4.0 (compatible; MSIE 5.5; Windows 98)\"\"\"\n",
      "68.233.81.168,\"-\",\"-\",6,\"Mar\",2005,13,56,46,\"-0500\",\"GET\",\"/scripts/root.exe?/c+dir\",\"HTTP/1.0\",404,1041,\"\"\"-\"\"\",\"\"\"-\"\"\"\n",
      "68.233.81.168,\"-\",\"-\",6,\"Mar\",2005,13,56,46,\"-0500\",\"GET\",\"/MSADC/root.exe?/c+dir\",\"HTTP/1.0\",404,1041,\"\"\"-\"\"\",\"\"\"-\"\"\"\n",
      "2019-11-27 23:42:26 ip-10-10-1-118 ess[3388]: ***Error*** cat: write error: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "# Create a column spec\n",
    "cols=\"IP:ip sep:' ' S:hypen sep:' ' S:user_id sep:' [' I:date sep:'/' I:month sep:'/' I:year sep:':' I:hour sep:':' I:minute sep:':' I:second sep:' 'S:sign sep:'' I:diff sep:'] \\\"' S:request sep:'\\\" ' I:status sep:' ' I:size sep:' ' S:f\"\n",
    "ess stream weblogs \"*\" \"*\" | aq_pp -f,+1,div,eok - \\\n",
    "-d IP:ip sep:' ' S:hypen sep:' ' S:user_id sep:' [' I:date sep:'/' S:month sep:'/' I:year sep:':' I:hour sep:':' I:minute sep:':' I:second sep:' ' S:time_dif sep:'] \"' S:request_method sep:' ' S:requested_resource sep:' ' S:protocol sep:'\" ' I:return_code sep:' ' I:obj_size sep:' ' S:referrer sep:' ' S:user_agent | \\\n",
    "head -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n",
    "\n",
    "Now we've done the cleaning and structuring of the data, we can move onto exploratory data analysis. \n",
    "\n",
    "Let's start with analyzing clients' ip column.\n",
    "\n",
    "First we'll take a look at frequency counts of each clients' IP addresses to identify where we're getting accessed from.\n",
    "\n",
    "# START BY FIXING THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<stdin>: Syntax error: byte=1+0 rec=1 field=#1\n",
      "<stdin>: Column spec not found or invalid\n",
      "Invalid parameter \"-\": ... -f,+1,aq - -o,csv -\n",
      "aq_pp: Input processing error\n",
      "2019-11-28 00:19:45 ip-10-10-1-118 ess[4130]: ***Error*** cat: write error: Broken pipe\n"
     ]
    },
    {
     "ename": "",
     "evalue": "13",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "# stream with ess, structurize with aq_pp, then count with aq_cnt\n",
    "# note that aq_pp is outputting the data in aq_tools' binary format, so that column specs do not need to be specified in aq_cnt\n",
    "ess stream weblogs \"*\" \"*\" | aq_pp -f,+1,div,eok - \\\n",
    "-d IP:ip sep:' ' S:hypen sep:' ' S:user_id sep:' [' I:date sep:'/' S:month sep:'/' I:year sep:':' I:hour sep:':' I:minute sep:':' I:second sep:' ' S:time_dif sep:'] \"' S:request_method sep:' ' S:requested_resource sep:' ' S:protocol sep:'\" ' I:return_code sep:' ' I:obj_size sep:' ' S:referrer sep:' ' S:user_agent -o,aq - | \\\n",
    "aq_pp -f,+1,aq - -o,csv - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
