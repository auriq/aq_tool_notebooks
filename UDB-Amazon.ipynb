{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UDB Walk Through with Amazon Product Dataset\n",
    "\n",
    "## Intro\n",
    "\n",
    "In this notebook we will be looking at a simple use case of udb, table, vector and variables to organize and perform little analysis on the [Amazon Product Review Dataset](https://s3.amazonaws.com/amazon-reviews-pds/readme.html), which is publically available on aws s3 bucket. \n",
    "\n",
    "If you're not familiar with the data, go ahead and take a look at the link above. \n",
    "\n",
    "## Pre-requisites\n",
    "\n",
    "You're assumed to have some knowledge of \n",
    "* bash and linux\n",
    "* Essentia and aq_commands\n",
    "* UDB, aq_udb command\n",
    "\n",
    "If you're not confident enough, you can take a look at the other notebooks, or look at [AuriQ Knowledge Base](http://auriq.com/knowledge-base/) where documentations and tutorials are available. \n",
    "\n",
    "Now that is out of the way, let's get started. \n",
    "\n",
    "## Dataset\n",
    "\n",
    "Before getting our hands dirty with the data, we'll take a look at the schema and what's inside of the data. \n",
    "This dataset contains customer reviews from 1995 - 2015, left on amazon.com, and contains 15 columns, some of which are important for this tutorial.\n",
    "* **customer_id** - unique identifier for a customer who left review(s)\n",
    "* **product_parent** - unique identifier for a product. This can be used to merge same product across different marketplace.\n",
    "* **review_id** - unique identifier for a review. \n",
    "* **star_rating** - star rating, ranging from 1 to 5. \n",
    "* **verified_purchase** - whether or not the review is based on verified purchase. String value of Y or N.\n",
    "* **review_date** - string value describing the year, month and date of the review. \n",
    "\n",
    "## What's UDB?\n",
    "If you're already familiar with these concepts, feel free to skip to next section (GOAL). \n",
    "\n",
    "What were the udb? It is a in-memory database that stores data using key-value type data structure. \n",
    "There are 3 data structures, and couple attributes to keep in mind.\n",
    "\n",
    "### Structures\n",
    "Because the database is key-value based, each database needs to have a primary key column specified, which is common across all the data structures within the database. \n",
    "* **Table** - Analogous to SQL table with primary key column, except that foregin key does not exist.\n",
    "* **Vector** - You can think of this as a row / column vector-ish data structure, if you're familiar with matrices. Each vector corresponds to a summary information of a primary key. Can be used to summarize customer information across entire dataset, for e.g. \n",
    "* **Variable** - as the name suggest, this stores values temporarly. Not really a datatype, but comes in handy when performing complex calculations. \n",
    "\n",
    "### Attributes\n",
    "UDB attributes are very powerful. You can assgin one of the following attributes to each column, and when the data flows into table/vector whose columns have these attributes, \n",
    "* `pkey`: primary hash key, must be string type\n",
    "* `tkey`: integer sorting key\n",
    "* `+key`: string key to merge on - only applicable for table. **ADD MORE EXPLANATION**\n",
    "* `+first`: Use the first imported value when merging\n",
    "* `+last`: Use the last imported values when merging\n",
    "* `+add`: Sum values across rows for each unique value\n",
    "* `+bor`: Bitwise-OR numeric values\n",
    "* `+min`: Take the smallest value\n",
    "* `+max`: Take the largest value\n",
    "* `+nozero`: Ignore values of 0 or an empty string\n",
    "\n",
    "## Goal\n",
    "\n",
    "The final goal of this tutorial is to import the data into udb, in a way that is easy to manage and analyze later. Concretely, we will create the following udb databases, tables, vectors and variables.\n",
    "\n",
    "### Database: Amazon\n",
    "This database contains a table and a vector, which are \n",
    "* Table: reviews - keeps all of the original review dataset. \n",
    "* vector: customer - contains summary of each customer's information, such as numbers of reviews each customer left, average star rating of each customer, numbers of helpful votes. \n",
    "\n",
    "**Put shemas here**\n",
    "\n",
    "### Database: Products\n",
    "This database only contains one vector\n",
    "* vector: product - summarizes the information for each product, such as numbers of reviews left, average star ratings, etc. \n",
    "\n",
    "**Schema**\n",
    "\n",
    "## Steps\n",
    "The whole project can be divided up into the following steps. \n",
    "1. define, and crate data schemas on udb, and start udb server\n",
    "2. Stream the data from datastore, process some of the columns, and fill up `reviews` table. \n",
    "3. **Update this section** Do some calculation and fill up the `customer` and `product` vector.\n",
    "\n",
    "## 1. definition \n",
    "We'll select datastore, create category, and define data schemas. Finally start the udb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 230M Nov 24 2017     /tsv/amazon_reviews_multilingual_DE_v1_00.tsv.gz\n",
      "  67M Nov 24 2017     /tsv/amazon_reviews_multilingual_FR_v1_00.tsv.gz\n",
      "  90M Nov 24 2017     /tsv/amazon_reviews_multilingual_JP_v1_00.tsv.gz\n",
      " 333M Nov 24 2017     /tsv/amazon_reviews_multilingual_UK_v1_00.tsv.gz\n",
      " 1.4G Nov 24 2017     /tsv/amazon_reviews_multilingual_US_v1_00.tsv.gz\n",
      "2020-03-04 19:43:23 ip-10-10-1-118 ess[3339]: Fetching file list from datastore.\n",
      "2020-03-04 19:43:23 ip-10-10-1-118 ess[3339]: Examining largest matched file to determine compression type: /tsv/amazon_reviews_multilingual_DE_v1_00.tsv.gz\n"
     ]
    }
   ],
   "source": [
    "# choosing the public s3 bucket that stores amazon review dataset as datastore\n",
    "ess select s3://amazon-reviews-pds\n",
    "ess ls /tsv/amazon_reviews_multi*.tsv.gz\n",
    "\n",
    "# create a category only including Danish reviews.\n",
    "ess category add danish_reviews \"/tsv/amazon_reviews_multilingual_DE_v1_00.tsv.gz\" --noprobe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:        danish_reviews\n",
      "Pattern:     /tsv/amazon_reviews_multilingual_DE_v1_00.tsv.gz\n",
      "Exclude:     None\n",
      "Date Format: auto\n",
      "Date Regex:  \n",
      "Archive:     \n",
      "Delimiter:   \n",
      "# of files:  1\n",
      "Total size:  230.7MB\n",
      "File range:  1970-01-01 - 1970-01-01\n",
      "# columns:   0\n",
      "Column Spec: \n",
      "Pkey: \n",
      "Schema: None\n",
      "Preprocess:  \n",
      "usecache:    False\n",
      "Comment:    \n",
      "\n",
      "First few lines:\n",
      "\u001f\u0000\u0000\u0000\u0000\u0000\u0000\u0000��r�X�6v\n",
      "=\u0005�2��\u001c",
      "��\u0000bϖ�-6�{p\n",
      "�@D�\u0002K\f",
      "\u0016��wЅ�eֺ�\u001b�@]Օ�Bz\u0004���\u0007\u00003��g���_&e-�\u0005\u0001������\u001e",
      "y���7���[OE�'���5�[��\u001c",
      "�/�j�&��)���R?��o� \u000f��'/��I�je���5�� ^Z+?�,���I�gV����9�}��O�E����)ҧ�����+ߛ��B��%�W���\u001e",
      "�?�'��p�M���|��?k\u001c",
      "8{Î5l4Z׃��c�jw�ݖ��4�a�=���ө�6����<͋x�-����ܞ\u0007ˀ^ʎ�0\u000fv�O^\u001c",
      "��u@�\u000f#Zڳ\u001fa�m�A��Y\u000fִ��\u0005c?\f",
      "��_�<Iֶg�AN��gAnGI���ʏ�����O�?���s�5��}�gEno������\u0017/���^&��~J�y�\u0007Il{�\u001c",
      "����'�nD4\f",
      "_���\u0013~��pZ;��N�aZ�;�k]��Nǝ�������Z�F�qݖ���ۍv�ճ��w+�\u000f3�<��ݰȾ�{�sR�A��gE�aaW�����O���\u001e",
      "\u0014� �G� �\u0013\u0003��}&�&댨��?��~�{9- �\u0004��W��|ۏL^�4Y���\f",
      "D+^T��״�M�\u0013C���\"Imz\u0001{��^w�)ȟ,ʫ3��'��K�\u0017��\u0017���3E�kӓV��/?o�\u0004a�\u000f��@\u001b��F\u0005���\u001b��D��;�V��������0j\\��F���\u001d",
      "�&��tڭ^��[�^�/��̶��2\u0000�� �=J���]*�dXG�T��\u0016�^~M�]�0�i�\u0019N�MG�.\u0002C��#�ڽ9�,�\u001f���\u001a\u000b",
      "��\u0005�N� ���{�����\u001d",
      "n\u001f�D�蕾6/\u0000\u001aז�\u001d",
      "rt�֕s;=8<r�\u000e\u000e��\u0013��Ncz��nY͆�8�V�k=��\u0001���ӑ��koi�\u0006s?�Ƿ���\u000b",
      "�\u001b-z\u0011�\u0011�A:E��G_�+oF\\N;3�u\u0007yf?��B?���i�C�4�kI?��*�\u0018/\u001e",
      "m���y���=\u000e|{ߧsNw�e��\u001c",
      "�?�sz^�=��i�oA��(�\u0017￮B?���g�����\u001d",
      "]��rp\u0017>�uD$\u0014�\u0001�\u0002Ve��l��4��������t�?o�\u001d",
      "Ni���`E�CMl��͏��Wd����Ok��\u0003\u001f�{��\"��Ks0P��}+�����_bz����s:���x�m�%\u001b/��%\u0017�6;�Hv�f�\u000b",
      "\u000b",
      "�~%+\u001f�D�l�\u0016���{��itw���-����GÓ�����d�8��N\u001e",
      ".;�ۑ���z�&�=��ARd���T^\u000b",
      "+\u0011��$��m�ˣ#f;;=��I@k���D\u0004ٶ��Ї�'\u0012\u0019u��[N�Y��+\u0016�Й��O[\u0016�@_\u0010\u0007�v\u0006Qk���RZY���vF��\u000b",
      "\u0015�̞�\u001e",
      "��&�J�!\u0002\f",
      "IB����D\u0016�lF�\u000b",
      "�V�������\u0007�_��7՞>�\n",
      "����\u0017ˈ��GԌW�?��4�jb\u0002H�Ⱦ��\u0011�\u0000\u0019�^�o�-��K^�G�0�\u001e",
      "��H\u0002.��\u000e�6�\u00131�4�x��U�T�_?[y��^\u0006�����\u000ffo�.�]{BOx��� ��\u0007����_涟C���?\\�&y\u001e",
      "�k�x..3/�/\u0016�pԠj�t���p�H\u0014��`1Q\u0014wM�|!>�M|\u0015A\u0012�����}��v&�\u0004ĞD,���_\n",
      "AZ_��^�F$�X<�����/t��I�b�3I�V��$������ z�\u001d",
      "����I�$�],���1�����\f",
      "\u001c",
      "C\u001a�Oc��\u001c",
      "\":�|�� z�J�\u0002<��ק5�;zse���F+X��y�h�t��vxQ�O��\u0011㳕��p\u0001A�$q�l���{8ش\u0003�4:��\u0012��N6\"�����_y� �h� �nt���,\"UA̿��$�\u0007�b���nnF\u0007�n+���7��@\u0002�\u0011G�Ʌ�`:*�fK�\u0016Okb��\u0000�⋼�@�p\u0010@�d�y�_~\u0006�6AO�s�wf�u(\\�u\u00127͘\n"
     ]
    }
   ],
   "source": [
    "ess summary danish_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we did not scan the source files when creating the category, Column spec, shema and first few lines of the files are not available with `ess summary <categoryName>`.\n",
    "\n",
    "But we do know what it looks like, from [datasource's website](https://s3.amazonaws.com/amazon-reviews-pds/readme.html).\n",
    "\n",
    "With this in mind, we'll create databases/tables/vectors with schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ip-10-10-1-118: Starting udbd-10010.\n",
      "ip-10-10-1-118: udbd-10010 (3544) started.\n"
     ]
    }
   ],
   "source": [
    "# This section will create a database and its schemas\n",
    "\n",
    "# first make sure there's no existing udb / schemas. \n",
    "ess server reset\n",
    "\n",
    "# create database \"amazon\", on port 0. \n",
    "# Everything that'll be created after this will be inside of \"amazon\" database. (except ohter database)\n",
    "ess create database amazon --port 0\n",
    "\n",
    "# create tables and vectors, with column specs in amazon db\n",
    "ess create table reviews S:marketplace I,pkey:customer_id I:u_time S:review_id S:product_id I:product_parent S:product_title S:product_category I:star_rating I:helpful_votes I:total_votes S:vine S:verified_purchase S:review_headline S:review_body S:review_date I:year I:month I:day\n",
    "ess create vector customer I,pkey:customer_id I:num_review I,+add:helpful_votes I,+add:total_votes I,+max:max_star I,+min:min_star F:avg_star I:verified_purchases\n",
    "\n",
    "# create products db, and product vector inside\n",
    "ess create database products\n",
    "ess create vector product I,pkey:product_parent I,+add:num_review I,+add:num_verified_purchase I,+min:min_star I,+add:sum_star\n",
    "\n",
    "# this will start the udbd server. \n",
    "ess udbd start "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the servers running with necessary schemas, we'll stream our data,\n",
    "* datastore (s3) --> udb table (`amazon`). \n",
    "\n",
    "Within the stream, we will also process some columns and thier values using `aq_pp` command, concretely...\n",
    "* extract year, month and date from `review_date` column, and remap them onto individual new columns.\n",
    "* For `star_rating` column, \n",
    "    * create new `sum_star`, `min_star` columns which will be streamed into `product` vector. \n",
    "* create and assign `num_review` column value of 1\n",
    "* create `num_verified_purchase` column, and conditionally fill it's value using `-if -else` option. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
